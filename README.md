# ğŸ“âœ¨ EduMind AI - Adaptive Knowledge Tracing via Personalized Federated Learning (PFL)

This project simulates a cutting-edge **Personalized Federated Learning (PFL)** system designed for educational applications (**Knowledge Tracing**). Unlike traditional centralized models, this architecture trains a unique, highly accurate prediction model for every student while preserving user privacy ğŸ”’ and leveraging collective data for better generalization ğŸŒ.

The core principle demonstrated is **Transfer Learning (Federated Personalization)**: a Global Model captures general learning patterns, and then its weights are fine-tuned locally using each student's minimal personal data to create a high-fidelity **Personalized Local Model** ğŸ§ .

---

## ğŸš€ Key Features

- ğŸ§© **Model-Per-Student Personalization**: A unique `LogisticRegression` model is fine-tuned for each student to predict their probability of success on any given topic.  
- ğŸŒ **Federated Learning (PFL) Simulation**: Simulates training a shared Global Model followed by Local Model Personalization (Transfer Learning).  
- â±ï¸ **Adaptive Scheduling**: Generates personalized **Spaced Repetition** intervals based on a student-specific Forgetting Rate.  
- ğŸ›¡ï¸ **Robust Data Handling**: Ensures class variety (both successes âœ… and failures âŒ) exists in training data.  
- ğŸ” **Interpretability**: Visualizes the difference between Global Model weights and Local Model weights, explaining why each student's recommendations are unique.

---

## ğŸ› ï¸ Prerequisites

This simulation is written in Python and requires the standard data science stack:

```bash
pip install pandas numpy scikit-learn matplotlib seaborn

---

## âš™ï¸ How to Run the Simulation

1. ğŸ“‹ **Copy the Code**: Copy the full Python code block for the EduMind AI simulation into your environment (Google Colab, Jupyter Notebook, or any Python IDE).  
2. â–¶ï¸ **Run All Cells**: Execute the entire script sequentially. Running all cells will:  
   - Generate simulated student data with personalized learning and forgetting rates  
   - Train the **Global Model** for overall learning patterns  
   - Fine-tune **Local Models** for each student using only their personal data  
   - Compute adaptive review intervals based on each studentâ€™s ForgettingRate  
   - Produce visualizations showing differences in feature importance between global and local models  
   - Highlight topics that need more frequent review for each student

---

## ğŸ“Š Key Code Sections and Principles

### 1ï¸âƒ£ Data Simulation
- âš¡ **LearningRate**: How quickly a studentâ€™s mastery increases after success  
- ğŸ§  **ForgettingRate**: How quickly scores drop due to DaysSinceLast session  
- âœ… **Data Fixes**: Ensures every studentâ€™s local data contains both successes (Correct=1) and failures (Correct=0), which is necessary for binary classification

### 2ï¸âƒ£ Global Model Training
- ğŸ—ï¸ **Model**: `LogisticRegression` with `max_iter=500`  
- ğŸŒ **Principle**: Federated Averaging (FedAvg) baseline  
- Establishes a common baseline of knowledge and difficulty across all students

### 3ï¸âƒ£ Local Model Personalization
For each student:  
- ğŸ†• Initialize a new local model  
- ğŸ”„ Copy global model weights (`coef_` and `intercept_`) to the local model (Transfer Learning)  
- ğŸ“ Fine-tune using only that studentâ€™s subset of data  
- **Principle**: Local models start with global knowledge and adapt to individual patterns for high accuracy with minimal data

### 4ï¸âƒ£ Adaptive Recommendations
- ğŸ¯ **Weakest Link**: Topic with minimum predicted success probability (`preds.idxmin()`)  
- â³ **Spaced Repetition**: Interval (`review_days`) calculated using personalized ForgettingRate, enabling adaptive review

---

## ğŸ” Model Interpretability (Visualization)

- ğŸŒŸ **Global Weights**: Average feature importance (Topic_encoded, DaysSinceLast) for the entire cohort  
- ğŸ§‘â€ğŸ“ **Local Weights**: Personalized importance for each student (e.g., S5)  
- ğŸ“Š Visualizations highlight why some students need more frequent review than others

---

## ğŸ›‘ Notes and Limitations

- ğŸ”’ **Privacy**: Raw data never leaves the client device; only encrypted model updates are shared. This simulation emulates transfer and fine-tuning.  
- ğŸ§© **Model Simplification**: This simulation uses `LogisticRegression` because it is fast, stable, and easy to interpret, especially for small or synthetic datasets. Real-world Knowledge Tracing often uses Deep Knowledge Tracing (DKT) with RNNs or LSTMs to capture sequential learning patterns. LogisticRegression cannot model dependencies on previous topics.  
- ğŸ” **Federated Rounds**: True Personalized Federated Learning (PFL) is iterative. Multiple rounds of global aggregation collect updates from clients, followed by local fine-tuning on each studentâ€™s device. Each round improves both global generalization and local personalization. This simulation performs **one global training round and one local fine-tuning round** to illustrate the concept without multi-round complexity.

---

This completes the full EduMind AI README. Copy **Blocks 1, 2, and 3 in order** into a single `.md` file, and it will be a fully unbroken Markdown document.
